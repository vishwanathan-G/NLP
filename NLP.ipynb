{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPSuV9O7OdoR",
        "outputId": "525eaf09-e65e-454d-96b7-42f8f7f54e24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigrams: Counter({'this': 2, 'is': 2, 'text': 2, 'corpus': 2, '.': 2, 'a': 1, 'sample': 1, 'used': 1, 'to': 1, 'demonstrate': 1, 'processing': 1})\n",
            "\n",
            "Bigrams: Counter({('this', 'is'): 1, ('is', 'a'): 1, ('a', 'sample'): 1, ('sample', 'text'): 1, ('text', 'corpus'): 1, ('corpus', '.'): 1, ('.', 'this'): 1, ('this', 'corpus'): 1, ('corpus', 'is'): 1, ('is', 'used'): 1, ('used', 'to'): 1, ('to', 'demonstrate'): 1, ('demonstrate', 'text'): 1, ('text', 'processing'): 1, ('processing', '.'): 1})\n",
            "\n",
            "Trigrams: Counter({('this', 'is', 'a'): 1, ('is', 'a', 'sample'): 1, ('a', 'sample', 'text'): 1, ('sample', 'text', 'corpus'): 1, ('text', 'corpus', '.'): 1, ('corpus', '.', 'this'): 1, ('.', 'this', 'corpus'): 1, ('this', 'corpus', 'is'): 1, ('corpus', 'is', 'used'): 1, ('is', 'used', 'to'): 1, ('used', 'to', 'demonstrate'): 1, ('to', 'demonstrate', 'text'): 1, ('demonstrate', 'text', 'processing'): 1, ('text', 'processing', '.'): 1})\n",
            "\n",
            "Bigram Probabilities:\n",
            "P(is|this) = 0.5\n",
            "P(corpus|this) = 0.5\n",
            "P(a|is) = 0.5\n",
            "P(used|is) = 0.5\n",
            "P(sample|a) = 1.0\n",
            "P(text|sample) = 1.0\n",
            "P(corpus|text) = 0.5\n",
            "P(processing|text) = 0.5\n",
            "P(.|corpus) = 0.5\n",
            "P(is|corpus) = 0.5\n",
            "P(this|.) = 1.0\n",
            "P(to|used) = 1.0\n",
            "P(demonstrate|to) = 1.0\n",
            "P(text|demonstrate) = 1.0\n",
            "P(.|processing) = 1.0\n",
            "\n",
            "Next word predictions for 'this': ['is', 'corpus']\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter, defaultdict\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "import random\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample text corpus\n",
        "corpus = \"This is a sample text corpus. This corpus is used to demonstrate text processing.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = nltk.word_tokenize(corpus.lower())\n",
        "\n",
        "# Unigrams\n",
        "unigrams = Counter(tokens)\n",
        "print(\"Unigrams:\", unigrams)\n",
        "\n",
        "# Bigrams\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "bigram_counts = Counter(bigrams)\n",
        "print(\"\\nBigrams:\", bigram_counts)\n",
        "\n",
        "# Trigrams\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "trigram_counts = Counter(trigrams)\n",
        "print(\"\\nTrigrams:\", trigram_counts)\n",
        "\n",
        "# Bigram probabilities\n",
        "bigram_probabilities = defaultdict(lambda: defaultdict(int))\n",
        "for w1, w2 in bigrams:\n",
        "    bigram_probabilities[w1][w2] += 1\n",
        "\n",
        "for w1 in bigram_probabilities:\n",
        "    total_count = float(sum(bigram_probabilities[w1].values()))\n",
        "    for w2 in bigram_probabilities[w1]:\n",
        "        bigram_probabilities[w1][w2] /= total_count\n",
        "\n",
        "print(\"\\nBigram Probabilities:\")\n",
        "for w1 in bigram_probabilities:\n",
        "    for w2 in bigram_probabilities[w1]:\n",
        "        print(f\"P({w2}|{w1}) = {bigram_probabilities[w1][w2]}\")\n",
        "\n",
        "# Next word prediction function\n",
        "def predict_next_word(word, num_predictions=3):\n",
        "    if word in bigram_probabilities:\n",
        "        sorted_predictions = sorted(bigram_probabilities[word].items(), key=lambda item: item[1], reverse=True)\n",
        "        return [word for word, prob in sorted_predictions[:num_predictions]]\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "# Example: Predict next words for 'this'\n",
        "next_words = predict_next_word('this')\n",
        "print(\"\\nNext word predictions for 'this':\", next_words)\n"
      ]
    }
  ]
}